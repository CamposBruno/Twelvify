---
phase: 02-backend-integration-ai-simplification
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/index.ts
  - backend/src/config/env.ts
  - backend/src/config/constants.ts
  - backend/src/middleware/rateLimit.ts
  - backend/src/middleware/errorHandler.ts
  - backend/src/middleware/logging.ts
  - backend/src/routes/simplify.ts
  - backend/src/routes/health.ts
  - backend/src/services/aiClient.ts
  - backend/src/services/logger.ts
  - backend/src/utils/errors.ts
  - backend/src/utils/validate.ts
  - backend/src/utils/fingerprint.ts
  - backend/package.json
  - backend/tsconfig.json
  - backend/.env.example
  - backend/.gitignore
autonomous: true
requirements:
  - BACK-01
  - BACK-02
  - BACK-03
  - BACK-04

user_setup:
  - service: openai
    why: "AI API for text simplification"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Platform → API keys → Create new secret key"
    dashboard_config: []

must_haves:
  truths:
    - "POST /api/simplify accepts {text} and streams SSE chunks back"
    - "GET /health returns 200 JSON confirming server is alive"
    - "Rate limiting rejects requests with 429 after 100 requests/hour per fingerprint"
    - "No text content ever appears in log output — only hashed fingerprint, token count, and timestamps"
    - "API key lives only in backend environment; extension never sees it"
  artifacts:
    - path: "backend/src/index.ts"
      provides: "Express app startup with all middleware applied"
      min_lines: 40
    - path: "backend/src/routes/simplify.ts"
      provides: "POST /api/simplify route with SSE streaming and OpenAI integration"
      min_lines: 60
    - path: "backend/src/middleware/rateLimit.ts"
      provides: "express-rate-limit middleware with IP+User-Agent fingerprinting"
      contains: "keyGenerator"
    - path: "backend/src/services/aiClient.ts"
      provides: "OpenAI streaming client wrapper"
      contains: "stream: true"
    - path: "backend/src/services/logger.ts"
      provides: "Winston structured logger that never logs text"
      min_lines: 20
    - path: "backend/src/utils/validate.ts"
      provides: "Zod schema for request validation (text max 5000 chars)"
      contains: "max(5000"
    - path: "backend/.env.example"
      provides: "Template for required environment variables"
      contains: "OPENAI_API_KEY"
  key_links:
    - from: "backend/src/routes/simplify.ts"
      to: "backend/src/services/aiClient.ts"
      via: "streamSimplification() generator function"
      pattern: "streamSimplification"
    - from: "backend/src/routes/simplify.ts"
      to: "backend/src/middleware/rateLimit.ts"
      via: "Express middleware chain"
      pattern: "limiter"
    - from: "backend/src/services/logger.ts"
      to: "backend/src/routes/simplify.ts"
      via: "logger.info with sanitized metadata"
      pattern: "logger\\.info"
---

<objective>
Build the Express backend proxy that handles all AI API calls, implements anonymous rate limiting, enforces privacy logging, and streams responses via SSE to the extension.

Purpose: The extension never touches API keys or AI providers directly. This backend is the sole mediator — it validates requests, enforces limits, calls OpenAI, streams results, and logs only anonymous metadata.

Output: A deployable Node.js/Express server at `backend/` with working SSE streaming on POST /api/simplify, 100 req/hr hard rate limit per fingerprint, and zero text in logs.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-backend-integration-ai-simplification/02-RESEARCH.md
@.planning/phases/02-backend-integration-ai-simplification/02-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scaffold backend with Express, dependencies, and TypeScript config</name>
  <files>
    backend/package.json
    backend/tsconfig.json
    backend/.env.example
    backend/.gitignore
    backend/src/config/env.ts
    backend/src/config/constants.ts
    backend/src/utils/errors.ts
    backend/src/utils/fingerprint.ts
    backend/src/utils/validate.ts
  </files>
  <action>
    Create `backend/` directory with all scaffold files:

    **backend/package.json** — npm project with:
    - name: "twelvify-backend", version: "0.1.0"
    - scripts: { "build": "tsc", "start": "node dist/index.js", "dev": "ts-node src/index.ts" }
    - dependencies: express@^4.18, cors@^2.8, dotenv@^16, zod@^3.20, winston@^3.11, openai@^4.50, express-rate-limit@^8.2
    - devDependencies: typescript@^5, @types/node@^20, @types/express@^4, @types/cors@^2, ts-node@^10

    **backend/tsconfig.json** — strict TypeScript:
    - target: ES2022, module: CommonJS, outDir: ./dist, rootDir: ./src
    - strict: true, esModuleInterop: true, resolveJsonModule: true, skipLibCheck: true

    **backend/.env.example** — template (no real values):
    ```
    OPENAI_API_KEY=sk-...your-key-here
    PORT=3001
    ALLOWED_ORIGIN=chrome-extension://YOUR_EXTENSION_ID
    NODE_ENV=development
    ```

    **backend/.gitignore**:
    ```
    node_modules/
    dist/
    .env
    .env.local
    logs/
    ```

    **backend/src/config/env.ts** — validate env vars on startup using zod:
    ```typescript
    import { z } from 'zod';
    dotenv.config({ path: '.env.local' });
    const envSchema = z.object({
      OPENAI_API_KEY: z.string().min(1),
      PORT: z.string().default('3001'),
      ALLOWED_ORIGIN: z.string().default('*'),
      NODE_ENV: z.enum(['development', 'production', 'test']).default('development'),
    });
    export const env = envSchema.parse(process.env);
    ```
    Throw with a clear message if OPENAI_API_KEY missing — fail fast.

    **backend/src/config/constants.ts**:
    ```typescript
    export const RATE_LIMIT = {
      windowMs: 60 * 60 * 1000,  // 1 hour
      max: 100,                    // Hard limit: 100 req/hr (backend enforced)
    };
    export const OPENAI_TIMEOUT_MS = 10000;  // 10 seconds
    export const MAX_TEXT_CHARS = 5000;
    export const SSE_KEEPALIVE_MS = 15000;
    ```

    **backend/src/utils/errors.ts** — custom error classes:
    ```typescript
    export class ValidationError extends Error { constructor(msg: string) { super(msg); this.name = 'ValidationError'; } }
    export class RateLimitError extends Error { constructor(public resetAt: Date) { super('Rate limit exceeded'); this.name = 'RateLimitError'; } }
    export class AITimeoutError extends Error { constructor() { super('AI request timed out'); this.name = 'AITimeoutError'; } }
    ```

    **backend/src/utils/fingerprint.ts** — hash IP + User-Agent using built-in crypto:
    ```typescript
    import { createHash } from 'crypto';
    export function hashFingerprint(ip: string, userAgent: string): string {
      return createHash('sha256').update(`${ip}:${userAgent}`).digest('hex').slice(0, 16);
    }
    ```

    **backend/src/utils/validate.ts** — Zod schema for simplify endpoint:
    ```typescript
    import { z } from 'zod';
    export const SimplifyRequestSchema = z.object({
      text: z.string()
        .min(1, 'Text cannot be empty')
        .max(5000, 'Text exceeds 5000 characters — select a shorter passage'),
    });
    export type SimplifyRequest = z.infer<typeof SimplifyRequestSchema>;
    ```

    After creating all files, run `cd backend && npm install` to install dependencies.

    Verify: `cd backend && npx tsc --noEmit` exits 0 (no TypeScript errors in scaffold files).
  </action>
  <verify>
    cd /path/to/project/backend && npx tsc --noEmit
    Expected: exits 0, no errors
  </verify>
  <done>
    backend/ directory exists with package.json, tsconfig.json, .env.example, .gitignore, and all src/config/ + src/utils/ files. npm install completed. tsc --noEmit passes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement OpenAI client, logger, rate limiter, middleware, routes, and Express app</name>
  <files>
    backend/src/services/aiClient.ts
    backend/src/services/logger.ts
    backend/src/middleware/rateLimit.ts
    backend/src/middleware/errorHandler.ts
    backend/src/middleware/logging.ts
    backend/src/routes/simplify.ts
    backend/src/routes/health.ts
    backend/src/index.ts
  </files>
  <action>
    Implement remaining backend files:

    **backend/src/services/logger.ts** — Winston with privacy-safe format:
    ```typescript
    import winston from 'winston';
    export const logger = winston.createLogger({
      level: process.env.NODE_ENV === 'production' ? 'info' : 'debug',
      format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json(),
      ),
      transports: [new winston.transports.Console()],
    });
    // NEVER log: text, req.body, user emails, raw IP, or any PII
    // Safe to log: fingerprint hash, token counts, response times, status codes
    ```

    **backend/src/services/aiClient.ts** — OpenAI streaming wrapper:
    ```typescript
    import OpenAI from 'openai';
    import { env } from '../config/env';
    import { OPENAI_TIMEOUT_MS } from '../config/constants';
    const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY, timeout: OPENAI_TIMEOUT_MS });

    const SYSTEM_PROMPT = `You are a text simplification assistant. When given text, rewrite it so it's clear and easy to understand:
    - Match the source language — simplify in whatever language the input is written in
    - Preserve structure: paragraphs, bullet points, and headings stay intact
    - Replace jargon with plain language (e.g. "myocardial infarction" → "heart attack")
    - Keep numbers, dates, proper nouns (names, places, brands) unchanged
    - Preserve code snippets, formulas, and technical notation exactly — only simplify surrounding prose
    - Preserve rich formatting hints (bolded words, links) where possible
    - Always use casual, friendly tone regardless of source formality
    - Adjust simplification intensity based on source complexity — light edits for simple text, heavy rewrite for dense text
    - Output ONLY the simplified text — no preamble, no explanations, no quotes`;

    export async function* streamSimplification(text: string): AsyncGenerator<string> {
      const stream = await openai.chat.completions.create({
        model: 'gpt-4o-mini',  // Cost-efficient; upgrade to gpt-4-turbo if quality insufficient
        messages: [
          { role: 'system', content: SYSTEM_PROMPT },
          { role: 'user', content: text },
        ],
        stream: true,
        max_tokens: 2000,
      });
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content;
        if (content) yield content;
      }
    }
    ```

    **backend/src/middleware/rateLimit.ts**:
    ```typescript
    import rateLimit from 'express-rate-limit';
    import { hashFingerprint } from '../utils/fingerprint';
    import { RATE_LIMIT } from '../config/constants';
    export const rateLimiter = rateLimit({
      windowMs: RATE_LIMIT.windowMs,
      max: RATE_LIMIT.max,
      standardHeaders: true,
      legacyHeaders: false,
      keyGenerator: (req) => hashFingerprint(req.ip ?? '0.0.0.0', req.get('user-agent') ?? ''),
      handler: (req, res) => {
        const resetAt = req.rateLimit.resetTime ?? new Date(Date.now() + RATE_LIMIT.windowMs);
        const retryAfterSeconds = Math.ceil((resetAt.getTime() - Date.now()) / 1000);
        const retryAfterMinutes = Math.ceil(retryAfterSeconds / 60);
        res.status(429).json({
          error: 'rate_limit_exceeded',
          message: `Chill, I need a break. Try again in ${retryAfterMinutes} minute${retryAfterMinutes === 1 ? '' : 's'}.`,
          resetAt: resetAt.toISOString(),
          retryAfterSeconds,
        });
      },
    });
    ```

    **backend/src/middleware/errorHandler.ts** — global Express error handler:
    ```typescript
    import { Request, Response, NextFunction } from 'express';
    import { logger } from '../services/logger';
    export function errorHandler(err: Error, req: Request, res: Response, _next: NextFunction) {
      logger.error('request_error', { name: err.name, message: err.message });
      if (res.headersSent) return;
      res.status(500).json({ error: 'internal_error', message: 'Something broke on my end. Try again?' });
    }
    ```

    **backend/src/middleware/logging.ts** — request logging middleware (no body):
    ```typescript
    import { Request, Response, NextFunction } from 'express';
    import { logger } from '../services/logger';
    import { hashFingerprint } from '../utils/fingerprint';
    export function requestLogger(req: Request, res: Response, next: NextFunction) {
      const start = Date.now();
      const fingerprint = hashFingerprint(req.ip ?? '0.0.0.0', req.get('user-agent') ?? '');
      res.on('finish', () => {
        logger.info('request', {
          method: req.method,
          path: req.path,
          status: res.statusCode,
          durationMs: Date.now() - start,
          fingerprint,  // hashed — not IP
          // NEVER log: req.body, text content, raw IP
        });
      });
      next();
    }
    ```

    **backend/src/routes/health.ts**:
    ```typescript
    import { Router } from 'express';
    export const healthRouter = Router();
    healthRouter.get('/health', (_req, res) => {
      res.json({ status: 'ok', timestamp: new Date().toISOString() });
    });
    ```

    **backend/src/routes/simplify.ts** — SSE streaming endpoint:
    ```typescript
    import { Router, Request, Response } from 'express';
    import { SimplifyRequestSchema } from '../utils/validate';
    import { streamSimplification } from '../services/aiClient';
    import { logger } from '../services/logger';
    import { hashFingerprint } from '../utils/fingerprint';
    import { rateLimiter } from '../middleware/rateLimit';

    export const simplifyRouter = Router();

    simplifyRouter.post('/api/simplify', rateLimiter, async (req: Request, res: Response) => {
      // 1. Validate request body
      const parsed = SimplifyRequestSchema.safeParse(req.body);
      if (!parsed.success) {
        const firstError = parsed.error.errors[0];
        const isTooLong = firstError.path[0] === 'text' && firstError.message.includes('5000');
        res.status(400).json({
          error: isTooLong ? 'text_too_long' : 'validation_error',
          message: isTooLong
            ? 'Easy there, speed racer. That\'s too much to chew. Select a shorter passage (under 5000 characters).'
            : firstError.message,
        });
        return;
      }

      const { text } = parsed.data;
      const fingerprint = hashFingerprint(req.ip ?? '0.0.0.0', req.get('user-agent') ?? '');
      const startTime = Date.now();

      // 2. Set SSE headers
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');
      res.setHeader('X-Accel-Buffering', 'no');  // Prevent nginx buffering
      res.flushHeaders();

      // 3. Set response timeout (10s max)
      res.setTimeout(10000, () => {
        res.write(`data: ${JSON.stringify({ error: 'timeout', message: 'That took too long. Hit me again?' })}\n\n`);
        res.end();
      });

      try {
        // 4. Stream from OpenAI token by token
        let tokenCount = 0;
        for await (const chunk of streamSimplification(text)) {
          res.write(`data: ${JSON.stringify({ chunk })}\n\n`);
          tokenCount += chunk.split(/\s+/).length;  // Approximate word count
        }

        // 5. Signal completion
        res.write(`data: ${JSON.stringify({ done: true })}\n\n`);
        res.end();

        // 6. Log metadata (never text content)
        logger.info('simplify_complete', {
          fingerprint,
          inputLengthBin: text.length < 500 ? 'short' : text.length < 2000 ? 'medium' : 'long',
          approxOutputWords: tokenCount,
          durationMs: Date.now() - startTime,
        });
      } catch (err: unknown) {
        const errorName = (err as Error).name ?? 'Error';
        logger.error('simplify_error', { fingerprint, errorName, durationMs: Date.now() - startTime });
        if (!res.headersSent) {
          res.write(`data: ${JSON.stringify({ error: 'ai_error', message: 'Something broke. Try again?' })}\n\n`);
        } else {
          res.write(`data: ${JSON.stringify({ error: 'ai_error', message: 'Something broke. Try again?' })}\n\n`);
        }
        res.end();
      }
    });
    ```

    **backend/src/index.ts** — Express app assembly:
    ```typescript
    import express from 'express';
    import cors from 'cors';
    import { env } from './config/env';
    import { requestLogger } from './middleware/logging';
    import { errorHandler } from './middleware/errorHandler';
    import { healthRouter } from './routes/health';
    import { simplifyRouter } from './routes/simplify';

    const app = express();

    // Trust proxy (for correct IP behind Vercel/Render/nginx)
    app.set('trust proxy', 1);

    app.use(cors({
      origin: env.ALLOWED_ORIGIN === '*' ? '*' : env.ALLOWED_ORIGIN,
      methods: ['GET', 'POST', 'HEAD', 'OPTIONS'],
    }));
    app.use(express.json({ limit: '64kb' }));
    app.use(requestLogger);

    app.use(healthRouter);
    app.use(simplifyRouter);
    app.use(errorHandler);

    const port = parseInt(env.PORT, 10);
    app.listen(port, () => {
      console.log(`Twelvify backend listening on port ${port}`);
    });
    ```

    After writing all files, create a `.env.local` file in backend/ for local development with a placeholder key (executor must note this requires a real OPENAI_API_KEY to function end-to-end):
    ```
    OPENAI_API_KEY=sk-placeholder-replace-with-real-key
    PORT=3001
    ALLOWED_ORIGIN=*
    NODE_ENV=development
    ```
    This file must be in .gitignore (already listed). Create it locally for build verification.

    Verify: `cd backend && npx tsc --noEmit` exits 0 — all TypeScript compiles without errors.
  </action>
  <verify>
    1. cd backend && npx tsc --noEmit → exits 0
    2. cd backend && node -e "require('./dist/index.js')" after npm run build → no import errors (or ts-node src/index.ts starts without crashing on import phase)
    3. Grep logs output: grep -r "req.body" backend/src/ → no matches (privacy check)
  </verify>
  <done>
    All backend source files exist. tsc --noEmit passes. No req.body logging anywhere in backend/src/. backend/.env.local exists locally with placeholder key. backend/.env.example has documentation for required vars.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `cd backend && npx tsc --noEmit` → exits 0
2. File structure matches research pattern: backend/src/{config,middleware,routes,services,utils}/
3. `grep -r "req\.body" backend/src/` → zero matches (BACK-03 privacy compliance)
4. `grep -r "text:" backend/src/services/logger.ts` → only metadata fields, no user text
5. `cat backend/src/utils/validate.ts | grep "max(5000"` → present (5000 char limit)
6. `cat backend/src/middleware/rateLimit.ts | grep "keyGenerator"` → present (fingerprint-based limiting)
7. `cat backend/src/routes/simplify.ts | grep "text/event-stream"` → present (SSE headers set)
</verification>

<success_criteria>
- POST /api/simplify endpoint exists with SSE streaming (BACK-01)
- express-rate-limit middleware with IP+User-Agent fingerprinting, 100 req/hr, returns 429 with resetAt (BACK-02)
- Winston logger emits only hashed fingerprint + token count + timestamps — never text content (BACK-03)
- HTTPS enforcement via CORS + server-side validation; .env.local keeps API key out of extension (BACK-04)
- TypeScript compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/02-backend-integration-ai-simplification/02-01-SUMMARY.md`
</output>
